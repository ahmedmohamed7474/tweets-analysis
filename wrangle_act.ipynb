{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419f6990",
   "metadata": {},
   "source": [
    "# WeRateDogs Data Cleaning and Analysis\n",
    "\n",
    "We will follow the define-code-test framework to clean the data according to tidy data principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47664318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Query Twitter API for each tweet in the Twitter archive and save JSON in a text file\n",
    "# These are hidden to comply with Twitter's API terms and conditions\n",
    "consumer_key = 'HIDDEN'\n",
    "consumer_secret = 'HIDDEN'\n",
    "access_token = 'HIDDEN'\n",
    "access_secret = 'HIDDEN'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "# NOTE TO STUDENT WITH MOBILE VERIFICATION ISSUES:\n",
    "# df_1 is a DataFrame with the twitter_archive_enhanced.csv file. You may have to\n",
    "# change line 17 to match the name of your DataFrame with twitter_archive_enhanced.csv\n",
    "# NOTE TO REVIEWER: this student had mobile verification issues so the following\n",
    "# Twitter API code was sent to this student from a Udacity instructor\n",
    "# Tweet IDs for which to gather additional data via Twitter's API\n",
    "tweet_ids = df_1.tweet_id.values\n",
    "len(tweet_ids)\n",
    "\n",
    "# Query Twitter's API for JSON data for each tweet ID in the Twitter archive\n",
    "count = 0\n",
    "fails_dict = {}\n",
    "start = timer()\n",
    "# Save each tweet's returned JSON as a new line in a .txt file\n",
    "with open('tweet_json.txt', 'w') as outfile:\n",
    "    # This loop will likely take 20-30 minutes to run because of Twitter's rate limit\n",
    "    for tweet_id in tweet_ids:\n",
    "        count += 1\n",
    "        print(str(count) + \": \" + str(tweet_id))\n",
    "        try:\n",
    "            tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "            print(\"Success\")\n",
    "            json.dump(tweet._json, outfile)\n",
    "            outfile.write('\\n')\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Fail\")\n",
    "            fails_dict[tweet_id] = e\n",
    "            pass\n",
    "end = timer()\n",
    "print(end - start)\n",
    "print(fails_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040324d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa33921f",
   "metadata": {},
   "source": [
    "## 1. Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e368927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data files\n",
    "twitter_archive = pd.read_csv('twitter-archive-enhanced.csv')\n",
    "image_predictions = pd.read_csv('image-predictions.tsv', sep='\\t')\n",
    "tweet_json = pd.read_json('tweet_json.txt', lines=True)[['id', 'retweet_count', 'favorite_count']].rename(columns={'id':'tweet_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d99c397",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_copy = twitter_archive.copy()\n",
    "image_predictions_copy = image_predictions.copy()\n",
    "tweet_json_copy = tweet_json.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b33078b",
   "metadata": {},
   "source": [
    "## 2. Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9c8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about datasets\n",
    "print('Twitter Archive Info:')\n",
    "twitter_archive.info()\n",
    "print('\\nImage Predictions Info:')\n",
    "image_predictions.info()\n",
    "print('\\nTweet JSON Info:')\n",
    "tweet_json.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd657293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values\n",
    "print('Missing values in Twitter Archive:')\n",
    "print(twitter_archive.isnull().sum())\n",
    "print('\\nMissing values in Image Predictions:')\n",
    "print(image_predictions.isnull().sum())\n",
    "print('\\nMissing values in Tweet JSON:')\n",
    "print(tweet_json.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd38afc3",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning\n",
    "### 3.1 Handle Dog Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fccdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dog_stages(df):\n",
    "    df_clean = df.copy()\n",
    "    stage_columns = ['doggo', 'floofer', 'pupper', 'puppo']\n",
    "    for col in stage_columns:\n",
    "        df_clean[col] = df_clean[col].replace('None', np.nan)\n",
    "    df_clean['dog_stage'] = df_clean[stage_columns].apply(lambda x: ', '.join(x.dropna()) if any(x.notna()) else np.nan, axis=1)\n",
    "    df_clean = df_clean.drop(columns=stage_columns)\n",
    "    return df_clean\n",
    "\n",
    "twitter_archive_clean = clean_dog_stages(twitter_archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c00a85e",
   "metadata": {},
   "source": [
    "### 3.2 Clean Rating Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b9418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ratings(df):\n",
    "    df_clean = df.copy()\n",
    "    df_clean['rating'] = df_clean.apply(lambda x: float(x['rating_numerator'])/float(x['rating_denominator']) if x['rating_denominator'] != 0 else np.nan, axis=1)\n",
    "    df_clean = df_clean.drop(columns=['rating_numerator', 'rating_denominator'])\n",
    "    return df_clean\n",
    "\n",
    "twitter_archive_clean = clean_ratings(twitter_archive_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a06d76e",
   "metadata": {},
   "source": [
    "### 3.3 Clean Timestamp Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f66402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_timestamps(df):\n",
    "    df_clean = df.copy()\n",
    "    df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'])\n",
    "    return df_clean\n",
    "\n",
    "twitter_archive_clean = clean_timestamps(twitter_archive_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c917c8cd",
   "metadata": {},
   "source": [
    "### 3.4 Clean Tweet Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3e183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sources(df):\n",
    "    df_clean = df.copy()\n",
    "    df_clean['source'] = df_clean['source'].str.extract(r'>(.*?)<')\n",
    "    return df_clean\n",
    "\n",
    "twitter_archive_clean = clean_sources(twitter_archive_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1831ba6",
   "metadata": {},
   "source": [
    "### 3.5 Clean Image Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da35763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_predictions(df):\n",
    "    df_clean = df.copy()\n",
    "    for col in ['p1', 'p2', 'p3']:\n",
    "        df_clean[col] = df_clean[col].str.replace('_', ' ').str.title()\n",
    "    conditions = [\n",
    "        (df_clean['p1_conf'] >= df_clean['p2_conf']) & (df_clean['p1_conf'] >= df_clean['p3_conf']),\n",
    "        (df_clean['p2_conf'] >= df_clean['p1_conf']) & (df_clean['p2_conf'] >= df_clean['p3_conf']),\n",
    "        (df_clean['p3_conf'] >= df_clean['p1_conf']) & (df_clean['p3_conf'] >= df_clean['p2_conf'])\n",
    "    ]\n",
    "    choices = [df_clean['p1'], df_clean['p2'], df_clean['p3']]\n",
    "    df_clean['predicted_breed'] = np.select(conditions, choices)\n",
    "    df_clean = df_clean[['tweet_id', 'predicted_breed', 'p1_dog', 'p2_dog', 'p3_dog']]\n",
    "    return df_clean\n",
    "\n",
    "image_predictions_clean = clean_predictions(image_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468be01f",
   "metadata": {},
   "source": [
    "## 4. Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889a4a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(twitter_df, image_df, json_df):\n",
    "    merged_df = twitter_df.merge(image_df, on='tweet_id', how='left')\n",
    "    merged_df = merged_df.merge(json_df, on='tweet_id', how='left')\n",
    "    merged_df = merged_df.dropna(subset=['rating'])\n",
    "    merged_df = merged_df.sort_values('timestamp')\n",
    "    return merged_df\n",
    "\n",
    "twitter_master = merge_datasets(twitter_archive_clean, image_predictions_clean, tweet_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783002a",
   "metadata": {},
   "source": [
    "## 5. Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eaff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned datasets\n",
    "twitter_archive_clean.to_csv('twitter_archive_clean.csv', index=False)\n",
    "image_predictions_clean.to_csv('image_predictions_clean.csv', index=False)\n",
    "twitter_master.to_csv('twitter_archive_master.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
